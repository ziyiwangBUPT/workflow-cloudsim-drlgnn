"""
éªŒè¯Task IDä¿®å¤

ä½¿ç”¨ä¸gym_envç›¸åŒçš„é€»è¾‘ï¼ŒéªŒè¯ä¿®å¤æ˜¯å¦æœ‰æ•ˆ
"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

from scheduler.dataset_generator.core.gen_dataset import generate_dataset
from scheduler.rl_model.core.types import TaskDto
from scheduler.rl_model.core.utils.task_mapper import TaskMapper
from scheduler.pre_scheduling.pre_computation import precompute_workflow_data
from scheduler.pre_scheduling.ws_method import ContentionAwareWorkflowSequencing
from scheduler.pre_scheduling.dp_method import BottleLayerAwareDeadlinePartition


# ç”Ÿæˆæ•°æ®é›†
dataset = generate_dataset(
    seed=42,
    host_count=4,
    vm_count=10,
    workflow_count=5,
    gnp_min_n=10,
    gnp_max_n=10,
    max_memory_gb=16,
    min_cpu_speed_mips=1000,
    max_cpu_speed_mips=3000,
    dag_method='gnp',
    task_length_dist='uniform',
    min_task_length=10000,
    max_task_length=100000,
    task_arrival='static',
    arrival_rate=1.0
)

print(f"ç”Ÿæˆäº† {len(dataset.workflows)} ä¸ªå·¥ä½œæµ")
print(f"åŸå§‹å·¥ä½œæµID: {[wf.id for wf in dataset.workflows]}")

# é¢„è°ƒåº¦
ws_scheduler = ContentionAwareWorkflowSequencing(alpha1=0.33, alpha2=0.33, alpha3=0.33)
dp_scheduler = BottleLayerAwareDeadlinePartition(beta=0.5)
rho = 0.2

for workflow in dataset.workflows:
    precompute_workflow_data(workflow, dataset.vms, rho)

sorted_workflows = ws_scheduler.run(dataset.workflows, dataset.vms)
print(f"WSæ’åºåå·¥ä½œæµID: {[wf.id for wf in sorted_workflows]}")

for workflow in sorted_workflows:
    dp_scheduler.run(workflow, dataset.vms)

# ğŸ”§ åº”ç”¨ä¿®å¤ï¼šé‡æ–°åˆ†é…workflow_id
print("\nåº”ç”¨ä¿®å¤ï¼šé‡æ–°åˆ†é…workflow_id")
for new_wf_id, workflow in enumerate(sorted_workflows):
    old_wf_id = workflow.id
    workflow.id = new_wf_id
    for task in workflow.tasks:
        task.workflow_id = new_wf_id
    print(f"  å·¥ä½œæµ: {old_wf_id} â†’ {new_wf_id}")

print(f"\nä¿®å¤åå·¥ä½œæµID: {[wf.id for wf in sorted_workflows]}")

dataset.workflows = sorted_workflows

# è½¬æ¢å’Œæ˜ å°„
tasks = [TaskDto.from_task(task) for workflow in dataset.workflows for task in workflow.tasks]
task_mapper = TaskMapper(tasks)
mapped_tasks = task_mapper.map_tasks()

print(f"\næ˜ å°„åä»»åŠ¡æ•°: {len(mapped_tasks)}")
print(f"_task_counts_cum: {task_mapper._task_counts_cum}")

# æ£€æŸ¥IDè¿ç»­æ€§
print("\næ£€æŸ¥IDè¿ç»­æ€§:")
errors = []
for i, task in enumerate(mapped_tasks):
    if task.id != i:
        errors.append((i, task.id))
        if len(errors) <= 5:
            print(f"  âŒ ç´¢å¼• {i}: ID={task.id} (æœŸæœ›{i})")
    elif i < 10:
        print(f"  âœ“ ç´¢å¼• {i}: ID={task.id}")

if not errors:
    print("\nâœ… æ‰€æœ‰ä»»åŠ¡IDè¿ç»­ï¼ä¿®å¤æˆåŠŸï¼")
else:
    print(f"\nâŒ å‘ç° {len(errors)} ä¸ªIDä¸åŒ¹é…")

