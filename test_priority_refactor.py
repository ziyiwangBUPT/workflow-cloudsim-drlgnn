"""
æµ‹è¯•å·¥ä½œæµä¼˜å…ˆçº§é‡æ„

éªŒè¯ï¼š
1. WSç®—æ³•åªè®¡ç®—åˆ†æ•°ä¸æ’åº
2. workflow_idä¿æŒä¸å˜
3. å…¨å±€ä»»åŠ¡ä¼˜å…ˆçº§æ­£ç¡®è®¡ç®—
4. æ—¶é’Ÿåˆå§‹åŒ–æ­£å¸¸å·¥ä½œ
5. ç¯å¢ƒåˆ›å»ºå’Œresetæ­£å¸¸
"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))


def test_ws_no_sorting():
    """æµ‹è¯•1: WSç®—æ³•ä¸æ’åº"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•1: WSç®—æ³•åªè®¡ç®—åˆ†æ•°ä¸æ’åº")
    print("=" * 80)
    
    from scheduler.dataset_generator.core.gen_dataset import generate_dataset
    from scheduler.pre_scheduling.pre_computation import precompute_workflow_data
    from scheduler.pre_scheduling.ws_method import ContentionAwareWorkflowSequencing
    
    # ç”Ÿæˆæ•°æ®é›†
    dataset = generate_dataset(
        seed=42,
        host_count=4,
        vm_count=10,
        workflow_count=5,
        gnp_min_n=8,
        gnp_max_n=8,
        max_memory_gb=16,
        min_cpu_speed_mips=1000,
        max_cpu_speed_mips=3000,
        dag_method='gnp',
        task_length_dist='uniform',
        min_task_length=10000,
        max_task_length=100000,
        task_arrival='static',
        arrival_rate=1.0
    )
    
    # ä¿å­˜åŸå§‹ID
    original_ids = [wf.id for wf in dataset.workflows]
    print(f"\nåŸå§‹å·¥ä½œæµID: {original_ids}")
    
    # é¢„è®¡ç®—
    for workflow in dataset.workflows:
        precompute_workflow_data(workflow, dataset.vms, 0.2)
    
    # è¿è¡ŒWSç®—æ³•
    ws_scheduler = ContentionAwareWorkflowSequencing(0.33, 0.33, 0.33)
    result_workflows = ws_scheduler.run(dataset.workflows, dataset.vms)
    
    # æ£€æŸ¥æ˜¯å¦æ’åº
    result_ids = [wf.id for wf in result_workflows]
    print(f"WSåå·¥ä½œæµID: {result_ids}")
    
    if original_ids == result_ids:
        print("âœ… å·¥ä½œæµIDé¡ºåºä¿æŒä¸å˜ï¼ˆæœªæ’åºï¼‰")
    else:
        print("âŒ å·¥ä½œæµIDé¡ºåºæ”¹å˜äº†ï¼ˆæ’åºäº†ï¼‰")
        return False
    
    # æ£€æŸ¥æ˜¯å¦è®¡ç®—äº†ä¼˜å…ˆçº§åˆ†æ•°
    print("\nå·¥ä½œæµä¼˜å…ˆçº§åˆ†æ•°:")
    for wf in result_workflows:
        print(f"  å·¥ä½œæµ {wf.id}: workflow_priority={wf.workflow_priority:.4f}")
        if wf.workflow_priority == 0.0:
            print(f"  âŒ å·¥ä½œæµ {wf.id} çš„ä¼˜å…ˆçº§åˆ†æ•°ä¸º0ï¼ˆæœªè®¡ç®—ï¼‰")
            return False
    
    print("\nâœ… æµ‹è¯•1é€šè¿‡ï¼šWSç®—æ³•åªè®¡ç®—åˆ†æ•°ä¸æ’åº\n")
    return True


def test_global_priority():
    """æµ‹è¯•2: å…¨å±€ä»»åŠ¡ä¼˜å…ˆçº§è®¡ç®—"""
    print("=" * 80)
    print("æµ‹è¯•2: å…¨å±€ä»»åŠ¡ä¼˜å…ˆçº§è®¡ç®—")
    print("=" * 80)
    
    from scheduler.dataset_generator.core.gen_dataset import generate_dataset
    from scheduler.pre_scheduling.pre_computation import precompute_workflow_data
    from scheduler.pre_scheduling.ws_method import ContentionAwareWorkflowSequencing
    from scheduler.pre_scheduling.dp_method import BottleLayerAwareDeadlinePartition
    
    # ç”Ÿæˆæ•°æ®é›†
    dataset = generate_dataset(
        seed=42,
        host_count=4,
        vm_count=10,
        workflow_count=3,
        gnp_min_n=5,
        gnp_max_n=5,
        max_memory_gb=16,
        min_cpu_speed_mips=1000,
        max_cpu_speed_mips=3000,
        dag_method='gnp',
        task_length_dist='uniform',
        min_task_length=10000,
        max_task_length=100000,
        task_arrival='static',
        arrival_rate=1.0
    )
    
    # é¢„è°ƒåº¦
    for workflow in dataset.workflows:
        precompute_workflow_data(workflow, dataset.vms, 0.2)
    
    ws_scheduler = ContentionAwareWorkflowSequencing(0.33, 0.33, 0.33)
    ws_scheduler.run(dataset.workflows, dataset.vms)
    
    dp_scheduler = BottleLayerAwareDeadlinePartition(0.5)
    for workflow in dataset.workflows:
        dp_scheduler.run(workflow, dataset.vms)
    
    # è®¡ç®—å…¨å±€ä»»åŠ¡ä¼˜å…ˆçº§ï¼ˆæ¨¡æ‹Ÿgym_envä¸­çš„é€»è¾‘ï¼‰
    for workflow in dataset.workflows:
        for task in workflow.tasks:
            task.global_priority = workflow.workflow_priority * task.rank_dp
    
    print("\nå…¨å±€ä»»åŠ¡ä¼˜å…ˆçº§ç¤ºä¾‹ï¼ˆå‰3ä¸ªå·¥ä½œæµçš„å‰2ä¸ªä»»åŠ¡ï¼‰:")
    for workflow in dataset.workflows[:3]:
        print(f"\nå·¥ä½œæµ {workflow.id} (workflow_priority={workflow.workflow_priority:.4f}):")
        for task in workflow.tasks[:2]:
            print(f"  ä»»åŠ¡ {task.id}:")
            print(f"    rank_dp={task.rank_dp:.2f}")
            print(f"    global_priority={task.global_priority:.4f}")
            
            # éªŒè¯è®¡ç®—æ­£ç¡®æ€§
            expected = workflow.workflow_priority * task.rank_dp
            if abs(task.global_priority - expected) > 1e-6:
                print(f"    âŒ å…¨å±€ä¼˜å…ˆçº§è®¡ç®—é”™è¯¯ï¼")
                return False
    
    print("\nâœ… æµ‹è¯•2é€šè¿‡ï¼šå…¨å±€ä»»åŠ¡ä¼˜å…ˆçº§è®¡ç®—æ­£ç¡®\n")
    return True


def test_env_creation():
    """æµ‹è¯•3: ç¯å¢ƒåˆ›å»ºå’Œreset"""
    print("=" * 80)
    print("æµ‹è¯•3: ç¯å¢ƒåˆ›å»ºå’Œreset")
    print("=" * 80)
    
    from scheduler.dataset_generator.core.gen_dataset import generate_dataset
    from scheduler.rl_model.core.env.gym_env import CloudSchedulingGymEnvironment
    
    # ç”Ÿæˆæ•°æ®é›†
    dataset = generate_dataset(
        seed=42,
        host_count=4,
        vm_count=10,
        workflow_count=3,
        gnp_min_n=5,
        gnp_max_n=5,
        max_memory_gb=16,
        min_cpu_speed_mips=1000,
        max_cpu_speed_mips=3000,
        dag_method='gnp',
        task_length_dist='uniform',
        min_task_length=10000,
        max_task_length=100000,
        task_arrival='static',
        arrival_rate=1.0
    )
    
    print("\nåˆ›å»ºç¯å¢ƒ...")
    env = CloudSchedulingGymEnvironment(dataset=dataset)
    
    print("æ‰§è¡Œreset...")
    obs, info = env.reset()
    
    print(f"âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    print(f"  ä»»åŠ¡æ•°: {len(obs.task_observations)}")
    print(f"  VMæ•°: {len(obs.vm_observations)}")
    
    # æ£€æŸ¥æ—¶é’Ÿç®¡ç†å™¨
    if env.state.clock_manager is not None:
        print(f"âœ… æ—¶é’Ÿç®¡ç†å™¨å·²åˆå§‹åŒ–")
        print(f"  å·¥ä½œæµæ—¶é’Ÿæ•°: {len(env.state.clock_manager.workflow_clocks)}")
        print(f"  ä»»åŠ¡æ˜ å°„æ•°: {len(env.state.clock_manager.task_to_workflow)}")
    else:
        print("âŒ æ—¶é’Ÿç®¡ç†å™¨æœªåˆå§‹åŒ–")
        return False
    
    print("\nâœ… æµ‹è¯•3é€šè¿‡ï¼šç¯å¢ƒåˆ›å»ºå’Œresetæ­£å¸¸\n")
    return True


def test_workflow_id_consistency():
    """æµ‹è¯•4: workflow_idä¸€è‡´æ€§"""
    print("=" * 80)
    print("æµ‹è¯•4: workflow_idä¸€è‡´æ€§")
    print("=" * 80)
    
    from scheduler.dataset_generator.core.gen_dataset import generate_dataset
    from scheduler.pre_scheduling.pre_computation import precompute_workflow_data
    from scheduler.pre_scheduling.ws_method import ContentionAwareWorkflowSequencing
    from scheduler.pre_scheduling.dp_method import BottleLayerAwareDeadlinePartition
    
    # ç”Ÿæˆæ•°æ®é›†
    dataset = generate_dataset(
        seed=42,
        host_count=4,
        vm_count=10,
        workflow_count=4,
        gnp_min_n=5,
        gnp_max_n=5,
        max_memory_gb=16,
        min_cpu_speed_mips=1000,
        max_cpu_speed_mips=3000,
        dag_method='gnp',
        task_length_dist='uniform',
        min_task_length=10000,
        max_task_length=100000,
        task_arrival='static',
        arrival_rate=1.0
    )
    
    # ä¿å­˜åŸå§‹IDå’Œä»»åŠ¡çš„workflow_id
    original_wf_ids = {wf.id: [task.workflow_id for task in wf.tasks] for wf in dataset.workflows}
    print(f"\nåŸå§‹workflow ID: {list(original_wf_ids.keys())}")
    
    # é¢„è°ƒåº¦
    for workflow in dataset.workflows:
        precompute_workflow_data(workflow, dataset.vms, 0.2)
    
    ws_scheduler = ContentionAwareWorkflowSequencing(0.33, 0.33, 0.33)
    ws_scheduler.run(dataset.workflows, dataset.vms)
    
    dp_scheduler = BottleLayerAwareDeadlinePartition(0.5)
    for workflow in dataset.workflows:
        dp_scheduler.run(workflow, dataset.vms)
    
    # è®¡ç®—å…¨å±€ä¼˜å…ˆçº§
    for workflow in dataset.workflows:
        for task in workflow.tasks:
            task.global_priority = workflow.workflow_priority * task.rank_dp
    
    # æ£€æŸ¥IDæ˜¯å¦ä¿æŒä¸å˜
    print(f"\né¢„è°ƒåº¦åworkflow ID: {[wf.id for wf in dataset.workflows]}")
    
    for wf in dataset.workflows:
        if wf.id not in original_wf_ids:
            print(f"âŒ å·¥ä½œæµ {wf.id} çš„IDæ”¹å˜äº†")
            return False
        
        # æ£€æŸ¥ä»»åŠ¡çš„workflow_id
        for task in wf.tasks:
            if task.workflow_id != wf.id:
                print(f"âŒ ä»»åŠ¡ {task.id} çš„workflow_idä¸ä¸€è‡´")
                return False
    
    print("âœ… æ‰€æœ‰workflow_idä¿æŒä¸å˜")
    print("âœ… æ‰€æœ‰ä»»åŠ¡çš„workflow_idä¸å…¶å·¥ä½œæµä¸€è‡´\n")
    
    print("\nâœ… æµ‹è¯•4é€šè¿‡ï¼šworkflow_idä¸€è‡´æ€§æ­£å¸¸\n")
    return True


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "*" * 80)
    print("å·¥ä½œæµä¼˜å…ˆçº§é‡æ„æµ‹è¯•")
    print("*" * 80)
    
    tests = [
        ("WSç®—æ³•ä¸æ’åº", test_ws_no_sorting),
        ("å…¨å±€ä»»åŠ¡ä¼˜å…ˆçº§è®¡ç®—", test_global_priority),
        ("ç¯å¢ƒåˆ›å»ºå’Œreset", test_env_creation),
        ("workflow_idä¸€è‡´æ€§", test_workflow_id_consistency),
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"\nâŒ æµ‹è¯• '{test_name}' å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            results.append((test_name, False))
    
    # æ€»ç»“
    print("=" * 80)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 80)
    
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{status}: {test_name}")
    
    all_passed = all(result for _, result in results)
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("\nâœ… é‡æ„æˆåŠŸï¼š")
        print("  1. WSç®—æ³•åªè®¡ç®—åˆ†æ•°ä¸æ’åº")
        print("  2. workflow_idä¿æŒä¸å˜")
        print("  3. å…¨å±€ä»»åŠ¡ä¼˜å…ˆçº§æ­£ç¡®è®¡ç®—")
        print("  4. æ—¶é’Ÿåˆå§‹åŒ–æ­£å¸¸å·¥ä½œ")
        print("  5. ç¯å¢ƒåˆ›å»ºå’Œresetæ­£å¸¸")
        return True
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

