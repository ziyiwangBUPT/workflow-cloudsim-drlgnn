# 新奖励函数实现总结（参考 ecmws 模式）

## 📋 概述

成功将 paper1115 的奖励函数从"改善量归一化模式"改为"ecmws 风格的真实值+惩罚模式"。

## 🎯 核心变化

### 旧奖励函数（改善量归一化）
```python
# 旧版本
makespan_reward = -(obs.makespan() - prev_obs.makespan()) / obs.makespan()
carbon_reward = -(obs.carbon_cost() - prev_obs.carbon_cost()) / obs.carbon_cost()
reward = makespan_reward + carbon_reward
```

**问题**：
- 奖励是相对改善量，不是绝对值
- makespan 和 carbon 都要归一化
- 难以直接反映真实成本

### 新奖励函数（ecmws 模式）
```python
# 新版本
task_obs = obs.task_observations[task_id]  # 获取当前调度的任务

# 基础奖励：负的碳成本（真实值）
reward = -task_obs.carbon_cost

# Deadline 惩罚
delta = task_obs.completion_time - task_obs.deadline
if delta > 0:  # 超时
    duration = task_obs.completion_time - task_obs.start_time
    reward = reward * (1.0 + delta / duration)  # 增加惩罚
```

**优势**：
- ✅ 直接使用真实碳成本值（gCO2）
- ✅ 超时惩罚机制（类似 ecmws）
- ✅ 奖励信号更直观
- ✅ 符合实际优化目标

## 📊 ecmws 奖励函数分析

### ecmws 原始代码
```python
def reward_fn(task_execution):
    task = task_execution.task
    delta = task.finish_time - task.deadline
    reward = - task_execution.electricity_cost  # 负的电价成本
    if delta <= 0:
        return reward  # 按时完成
    else:
        return reward * (1 + delta / (task.finish_time - task.start_time))  # 超时惩罚
```

### paper1115 实现对应
```python
# paper1115 版本
task_obs = obs.task_observations[task_id]
carbon_cost = task_obs.carbon_cost
reward = -carbon_cost  # 对应 -electricity_cost

delta = task_obs.completion_time - task_obs.deadline
if delta > 0:
    duration = task_obs.completion_time - task_obs.start_time
    reward = reward * (1.0 + delta / duration)  # 对应超时惩罚
```

## 🔧 实现细节

### 1. 碳强度数值调整

**文件**: `scheduler/config/carbon_intensity.py`

碳强度数值放大1000倍（用户已修改）：
```python
# 旧值（gCO2/kWh）
host1 = [0.15, 0.10, 0.11, ...]  

# 新值（放大1000倍）
host1 = [1500, 1000, 1100, ...]  
```

**原因**: 使碳成本的绝对值更有意义，方便训练

### 2. 奖励函数修改

**文件**: `scheduler/rl_model/agents/gin_agent/wrapper.py`

修改 `step()` 方法：
- 移除 makespan 奖励（不再关注完工时间改善）
- 移除碳成本归一化
- 添加 deadline 超时惩罚

```python
def step(self, action: int) -> tuple[np.ndarray, SupportsFloat, bool, bool, dict[str, Any]]:
    mapped_action = self.map_action(action)
    obs, _, terminated, truncated, info = super().step(mapped_action)
    
    # 获取当前调度的任务
    task_id = mapped_action.task_id
    task_obs = obs.task_observations[task_id]
    
    # 基础奖励：-碳成本
    reward = -task_obs.carbon_cost
    
    # Deadline 惩罚
    delta = task_obs.completion_time - task_obs.deadline
    if delta > 0:
        duration = task_obs.completion_time - task_obs.start_time
        if duration > 0:
            reward = reward * (1.0 + delta / duration)
    
    return mapped_obs, reward, terminated, truncated, info
```

## 📈 奖励函数公式

### 基本公式
```
reward = -carbon_cost × penalty_factor

其中：
  carbon_cost = 任务碳成本（gCO2，真实值）
  
  penalty_factor = {
    1.0,                        if 按时完成（delta <= 0）
    1.0 + delta / duration,     if 超时（delta > 0）
  }
  
  delta = completion_time - deadline  # 超时时间
  duration = completion_time - start_time  # 任务执行时长
```

### 奖励特性

1. **基础奖励始终为负** - 因为碳成本是惩罚
2. **按时完成** - reward = -carbon_cost（负值但较小）
3. **超时完成** - reward 变得更负（惩罚加重）
4. **超时越多，惩罚越大** - penalty_factor 随 delta 增长

### 示例计算

**情况1：按时完成**
```
carbon_cost = 0.05 gCO2
completion_time = 100s
deadline = 120s
delta = 100 - 120 = -20s  # 提前完成

reward = -0.05  # 仅碳成本惩罚
```

**情况2：轻微超时**
```
carbon_cost = 0.05 gCO2
start_time = 50s
completion_time = 130s
deadline = 120s
delta = 130 - 120 = 10s  # 超时10秒
duration = 130 - 50 = 80s

penalty_factor = 1.0 + 10/80 = 1.125
reward = -0.05 × 1.125 = -0.05625  # 惩罚加重12.5%
```

**情况3：严重超时**
```
carbon_cost = 0.05 gCO2
start_time = 50s
completion_time = 150s
deadline = 120s
delta = 150 - 120 = 30s  # 超时30秒
duration = 150 - 50 = 100s

penalty_factor = 1.0 + 30/100 = 1.3
reward = -0.05 × 1.3 = -0.065  # 惩罚加重30%
```

## ✅ 测试验证

### 测试脚本

1. **test_new_reward_function.py** - 新奖励函数逻辑测试
   - 测试1：奖励计算逻辑
   - 测试2：Deadline 惩罚机制
   - 测试3：碳成本数值范围

2. **train_mini_batch.py** - 小批次训练验证
   - 测试训练循环
   - 测试奖励梯度
   - 验证环境正常运行

### 运行测试

```bash
# 需要先安装 gymnasium
pip install gymnasium==0.28.1

# 运行奖励函数测试
python test_new_reward_function.py

# 运行训练验证
python train_mini_batch.py
```

## 🔍 与 ecmws 的对比

| 特性 | ecmws | paper1115 (新) |
|------|-------|----------------|
| 优化目标 | 电价成本 | 碳成本 |
| 基础奖励 | -electricity_cost | -carbon_cost |
| 时间约束 | task.deadline | task.deadline (DP算法生成) |
| 超时惩罚 | ✅ | ✅ (同样的公式) |
| 归一化 | ❌ (真实值) | ❌ (真实值) |
| 惩罚因子 | 1 + delta/duration | 1 + delta/duration |

## 📌 关键要点

### 1. 奖励是负值
- 所有奖励都 ≤ 0（成本/惩罚）
- 按时完成：较小的负值
- 超时：更大的负值

### 2. Deadline 来源
- 由预调度阶段的 DP (Deadline Partition) 算法生成
- 每个任务有自己的子截止时间
- 存储在 `task.deadline` 中

### 3. 碳成本计算
- 在 `gym_env.py` 的 `step()` 中自动计算
- 公式：`carbon_cost = energy(kWh) × avg_carbon_intensity(gCO2/kWh)`
- 保存在 `task_state.carbon_cost` 中

### 4. 训练意义
- 智能体学习选择低碳成本的 VM
- 智能体学习避免超时（deadline 约束）
- 平衡碳成本和完成时间

## 🚀 下一步

### 开始正式训练

1. **安装依赖**：
   ```bash
   pip install gymnasium==0.28.1
   ```

2. **运行训练脚本**（你的正式训练代码）

3. **监控指标**：
   - 平均奖励（应该逐渐增大，接近0）
   - 总碳成本（应该逐渐减小）
   - 按时完成率（应该逐渐提高）

### 可能的调整

1. **奖励权重**（如需要）：
   ```python
   # 在 wrapper.py 中
   reward = -alpha * carbon_cost  # alpha调整碳成本权重
   ```

2. **惩罚强度**（如需要）：
   ```python
   # 调整超时惩罚的强度
   penalty_factor = 1.0 + beta * (delta / duration)  # beta调整惩罚强度
   ```

3. **碳强度数值**：
   - 可以使用真实数据
   - 可以调整数值范围以适应训练

## 📁 修改文件清单

1. ✅ `scheduler/config/carbon_intensity.py` - 碳强度数值（用户已修改）
2. ✅ `scheduler/rl_model/agents/gin_agent/wrapper.py` - 奖励函数
3. ✅ `test_new_reward_function.py` - 奖励函数测试
4. ✅ `train_mini_batch.py` - 训练验证脚本

## 💡 训练建议

1. **初期训练**：
   - 使用小的数据集（少量工作流和任务）
   - 短 episode（快速验证）
   - 观察奖励趋势

2. **调试**：
   - 如果奖励不收敛，检查碳成本数值是否合理
   - 如果超时率太高，考虑调整 deadline 生成参数
   - 如果训练不稳定，考虑添加奖励剪裁

3. **正式训练**：
   - 增大数据集
   - 增加训练步数
   - 使用多个环境并行训练

---

**实现时间**: 2025年10月29日  
**实现状态**: ✅ 完成，待运行测试验证
**参考项目**: ecmws-experiments

